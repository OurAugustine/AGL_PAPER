{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fd4551",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "The last question is about the gradient descent of logistic regression.\n",
    "In the gitlab repository, there is a folder code including the gradient descent algorithm for linear regression. The task here is to write the gradient descent algorithm for the logistic regression. Requirement: the function is called LogisticRegression and is implemented in a class with the following structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return # the sigmoid function\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, M, N, lr = 0.1):\n",
    "        self.M  = M\n",
    "        self.N  = N\n",
    "        self.lr = lr\n",
    "        self.W  = np.zeros((N, 1))\n",
    "        self.b  = 0.\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y, epoch=2000, visual=False):\n",
    "        self.scaler.fit(X)\n",
    "        X = self.scaler.transform(X)\n",
    "\n",
    "        # perform the gradient descent\n",
    "        for i in range(epoch):\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        y_hat  = sigmoid(np.dot(X_test, self.W)+self.b)\n",
    "        return np.where(y_hat>0.5, 1, 0)\n",
    "\n",
    "    def loss_fcn(self, X, y):\n",
    "        y_hat  = sigmoid(np.dot(X, self.W) + self.b)\n",
    "        return -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/self.M\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f700f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd6c153d",
   "metadata": {},
   "source": [
    "## My Answer\n",
    "### Creating the sigmoid function\n",
    "Sigmoid Function: First, letâ€™s define the sigmoid function, which is used to transform the linear combination of features into probabilities. The sigmoid function is given by: [$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b77908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f4319",
   "metadata": {},
   "source": [
    "### Working on my fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d76a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LogisticRegression_A:\n",
    "    # initialization method\n",
    "    def __init__(self, M, N, lr = 0.1):\n",
    "        self.M  = M\n",
    "        self.N  = N\n",
    "        self.lr = lr\n",
    "        self.W  = np.zeros(N)#((N, 1))\n",
    "        self.b  = 0.\n",
    "        self.scaler = StandardScaler()  \n",
    "        \n",
    "\n",
    "    #the fit method\n",
    "    def fit(self, X, y, epoch=2000, visual=False):\n",
    "        self.scaler.fit(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        #N, M = X.shape\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Compute predictions\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_hat = sigmoid(z)\n",
    "\n",
    "            # Compute gradients\n",
    "            dW = np.dot(X.T, (y_hat - y)) /self.M #M\n",
    "            db = np.sum(y_hat - y) / self.M    #M\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.W -= self.lr * dW\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "            # Optional: Visualize loss during training\n",
    "            if visual:\n",
    "                loss = self.loss_fcn(X, y)\n",
    "                print(f\"Epoch {i+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # the predict and the loss fuction methods\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        y_hat  = sigmoid(np.dot(X_test, self.W)+self.b)\n",
    "        return np.where(y_hat>0.5, 1, 0)\n",
    "\n",
    "    def loss_fcn(self, X, y):\n",
    "        y_hat  = sigmoid(np.dot(X, self.W) + self.b)\n",
    "        return -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/self.M    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b504d",
   "metadata": {},
   "source": [
    "# Testing my code using some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e34f102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,) (20,)\n",
      "[1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "X = np.random.rand(100, 4)  # Features\n",
    "y = np.random.randint(0, 2, size=100)  # Binary labels (0 or 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train and y_test using ravel()\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "print(y_train.shape,y_test.shape)\n",
    "# Create and fit a logistic regression model\n",
    "mdl = my_LogisticRegression_A(M=X_train.shape[0],N=X_train.shape[1])\n",
    "mdl.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred0 = mdl.predict(X_test)\n",
    "\n",
    "# Evaluate the model (e.g., accuracy, precision, recall, etc.)\n",
    "# ...\n",
    "\n",
    "# Note: You don't need to reshape y_pred; it will have the correct shape automatically.\n",
    "print(y_pred0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc37c6",
   "metadata": {},
   "source": [
    "### This might be anoter way of coding it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbed3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class my_LogisticRegression_B:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        M, N = X.shape\n",
    "        self.W = np.zeros(N)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_hat = self.sigmoid(z)\n",
    "\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / M\n",
    "\n",
    "            # Compute gradients\n",
    "            dW = np.dot(X.T, (y_hat - y)) / M\n",
    "            db = np.sum(y_hat - y) / M\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Example usage\n",
    "X = np.random.rand(100, 2)  # Features\n",
    "y = np.random.randint(0, 2, size=100)  # Binary labels (0 or 1)\n",
    "\n",
    "model_B = my_LogisticRegression_B()\n",
    "model_B.fit(X, y)\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_B.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c6833",
   "metadata": {},
   "source": [
    "### Testing the code with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00d988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,) (20,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "X = np.random.rand(100, 4)  # Features\n",
    "y = np.random.randint(0, 2, size=100)  # Binary labels (0 or 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train and y_test using ravel()\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "print(y_train.shape,y_test.shape)\n",
    "# Create and fit a logistic regression model\n",
    "clf = my_LogisticRegression_B()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model (e.g., accuracy, precision, recall, etc.)\n",
    "# ...\n",
    "\n",
    "# Note: You don't need to reshape y_pred; it will have the correct shape automatically.\n",
    "print(y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa339f",
   "metadata": {},
   "source": [
    "#### This is one key error that i was a game changer in my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72e7bc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=20\n",
    "np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1211c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
