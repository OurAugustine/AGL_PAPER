{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b86498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74737e28",
   "metadata": {},
   "source": [
    "# 6. Gradient Boosting Decision Trees (GBDT)\n",
    "This lab is about the Gradient Boosting Decision Trees. We will demonstrate the GBDT algorithm using the `numpy` library with the Hubor loss function.\n",
    "Your task of this lab is to have a folder named `lab6` in your repository as `labs/lab6`. Inside the folder, you should have the python file named `decision_tree_regression.py` and `gradient_boosting_decision_tree.py`. The `decision_tree_regression.py` file should contain the decision tree regression class and is from your previous lab 5, and the `gradient_boosting_decision_tree.py` file should contain the gradient boosting decision tree class.\n",
    "* create the `decision_tree_regression.py` file and copy the decision tree regression class from your previous lab 5.\n",
    "* use the following code and complete the function `mae_gradient_descent` and `mae_loss_gradient` in the `gradient_boosting_decision_tree.py` file.\n",
    "* the goal is under the folder `labs/lab6`, you should be able to run the code by `python gradient_boosting_decision_tree.py` and get the output of the RMSE value.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12cc963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from LAB05_DECISION_TREE_REG import my_DecisionTreeReg as decisiontree\n",
    "from LAB05_DECISION_TREE_REG import RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7673c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.798461954479579\n"
     ]
    }
   ],
   "source": [
    "class GradientBoostingDecisionTree:\n",
    "    def __init__(self, max_depth=8, n_estimators=10, lr=0.01, max_features=5, delta=1.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr # Learning rate (shrinkage)\n",
    "        self.delta = delta # Huber loss delta parameter\n",
    "\n",
    "        self.gradient_coeff = []\n",
    "        self.stop = n_estimators\n",
    "\n",
    "        #self.tree0 = decisiontree(max_depth=self.max_depth, max_features=max_features)\n",
    "        #self.trees = [decisiontree(max_depth=self.max_depth, max_features=max_features) for _ in range(self.n_estimators)]\n",
    "        self.tree0 = decisiontree(max_depth=self.max_depth)\n",
    "        self.trees = [decisiontree(max_depth=self.max_depth) for _ in range(self.n_estimators)]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.M, self.N = X.shape\n",
    "\n",
    "        self.tree0.fit(X, y)\n",
    "        y_pred = self.tree0.predict(X).reshape((self.M, 1))\n",
    "        residue = y - y_pred\n",
    "\n",
    "        for idx, itree in enumerate(self.trees):\n",
    "            if np.linalg.norm(residue) < 1e-4:\n",
    "                self.stop = idx\n",
    "                break\n",
    "                \n",
    "            itree.fit(X, residue)\n",
    "            ipred = itree.predict(X).reshape((self.M, 1))\n",
    "\n",
    "            alpha = self.huber_gradient_descent(ipred, residue, self.lr, 150)\n",
    "            self.gradient_coeff.append(alpha)\n",
    "\n",
    "            y_pred = np.add(y_pred, self.lr * alpha * ipred)\n",
    "            residue = y - y_pred\n",
    "            # Update residue based on Huber loss gradient\n",
    "            residue = self.huber_loss_gradient(residue, self.delta)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.tree0.predict(X_test).reshape((-1, 1))\n",
    "        for idx, itree in enumerate(self.trees):\n",
    "            if idx == self.stop:\n",
    "                break\n",
    "            coeff = self.lr * self.gradient_coeff[idx]\n",
    "            y_pred = np.add(y_pred, coeff * itree.predict(X_test).reshape((-1, 1)))\n",
    "        return y_pred\n",
    "\n",
    "    def huber_gradient_descent(self, a, b, lr, epochs):\n",
    "        alpha = np.random.randn(1)[0]\n",
    "        for epoch in range(epochs):\n",
    "            # update alpha using the gradient of the Huber loss\n",
    "            grad = np.sum(self.huber_loss_gradient(b - a * alpha, self.delta) * (-a))\n",
    "            alpha -= lr * grad\n",
    "        return alpha\n",
    "\n",
    "    def huber_loss_gradient(self, a, delta):\n",
    "        # Gradient of the Huber loss\n",
    "        return np.where(np.abs(a) <= delta, -a, -delta * np.sign(a))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    X_train = pd.read_csv('airfoil/airfoil_self_noise_X_train.csv').values\n",
    "    y_train = pd.read_csv('airfoil/airfoil_self_noise_y_train.csv').values\n",
    "    X_test  = pd.read_csv('airfoil/airfoil_self_noise_X_test.csv').values\n",
    "    y_test  = pd.read_csv('airfoil/airfoil_self_noise_y_test.csv').values\n",
    "\n",
    "    GBDT = GradientBoostingDecisionTree(n_estimators=200, max_depth=15)\n",
    "    GBDT.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = GBDT.predict(X_test)\n",
    "    print(RMSE(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e304f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7103a734",
   "metadata": {},
   "source": [
    "### MY CODE WITH MSE \n",
    "#### importing my regressors from the previous labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f975dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from LAB05_DECISION_TREE_REG import my_DecisionTreeReg as DTR\n",
    "from LAB05_DECISION_TREE_REG import RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6282f28",
   "metadata": {},
   "source": [
    "####  loading my data set inside an independent set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a345d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('airfoil/airfoil_self_noise_X_train.csv').values\n",
    "y_train = pd.read_csv('airfoil/airfoil_self_noise_y_train.csv').values\n",
    "X_test  = pd.read_csv('airfoil/airfoil_self_noise_X_test.csv').values\n",
    "y_test  = pd.read_csv('airfoil/airfoil_self_noise_y_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d45aa",
   "metadata": {},
   "source": [
    "### implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39cf045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residual as the target values\n",
    "        residual = np.copy(y)\n",
    "        \n",
    "        # Train a sequence of decision trees\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Train a decision tree to predict the residual\n",
    "            tree = DTR(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            \n",
    "            # Update the predictions by adding the predictions of the new tree\n",
    "            predictions = tree.predict(X)\n",
    "            y_pred = np.sum([model.predict(X) for model in self.models], axis=0) + self.learning_rate * predictions\n",
    "            \n",
    "            # Update the residual\n",
    "            residual -= predictions\n",
    "                        \n",
    "            # Add the trained tree to the list of models\n",
    "            self.models.append(tree)\n",
    "            \n",
    "            # Adjust the learning rate\n",
    "            self.adjust_learning_rate(y_pred, residual, mse_gradient_factor=0.1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions of all trees\n",
    "        return np.sum([model.predict(X) for model in self.models], axis=0)\n",
    "    \n",
    "    \n",
    "    def adjust_learning_rate(self, y_pred, residual, mse_gradient_factor=0.1):\n",
    "        # Calculate the gradient of the mean squared error\n",
    "        mse_gradient = self._calculate_mse_gradient(y_pred, residual)\n",
    "        \n",
    "        # Update the learning rate based on the gradient of the mean squared error\n",
    "        self.learning_rate *= np.exp(-mse_gradient_factor * mse_gradient)\n",
    "    \n",
    "    def _calculate_mse_gradient(self, y_pred, residual):\n",
    "               \n",
    "        # Calculate the gradient of the mean squared error\n",
    "        mse_gradient = np.mean(2 * (y_pred - residual))\n",
    "        \n",
    "        return mse_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4892ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.766644215547662\n"
     ]
    }
   ],
   "source": [
    "GBDT2 = GradientBoostingRegressor(n_estimators=200, max_depth=15)\n",
    "GBDT2.fit(X_train, y_train.reshape(-1))\n",
    "y_pred = GBDT2.predict(X_test)\n",
    "print(RMSE(y_pred, y_test.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725740c",
   "metadata": {},
   "source": [
    "#### Just testining using other data sets that do not need reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fda7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 17251.487700933678\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate random regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train your Gradient Boosting Regressor model\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X_train1, y_train1)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred1 = gbr.predict(X_test1)\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(y_test1, y_pred1)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29441a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,X_train.shape,y.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8f2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
