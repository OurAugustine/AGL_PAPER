{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26877cc-5d8b-44f8-b621-de305b9652aa",
   "metadata": {},
   "source": [
    "# 9. Keras and deep learning\n",
    "In this lab, we will learn how to use Keras to build deep learning models. We will use Keras to build a LSTM model for sentiment classification and a CNN model for digit recognition.\n",
    "You need to put in the code to complete the models in the blocks marked with `## YOUR CODE HERE` and `## END OF YOUR CODE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883c76f-1649-4fca-a014-d2b9a43520d0",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before you can start using Keras, you'll need to install TensorFolw, which includes Keras as part of its core library.\n",
    "```bash\n",
    "source activate {your_env}\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baa88c4-c34c-4dba-8a7a-9d5578c6a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79e78b-b5cc-4aa8-a61e-12ad2fa2e3cf",
   "metadata": {},
   "source": [
    "## Basics of Keras\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "### 1. Initialize a model\n",
    "Start by creating a Sequential model and adding layers to it.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# this is equivalent to the above\n",
    "#model = Sequential([\n",
    "#    Dense(64, activation='relu', input_dim=100),\n",
    "#    Dense(10, activation='softmax')\n",
    "#])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241c2cdc-b3eb-45b7-a9db-dceeb103d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b149c-8a46-4ff8-ac2e-89816cb5a8a4",
   "metadata": {},
   "source": [
    "### 2. Compile the model\n",
    "Compile the model with the appropriate loss function and optimizer.\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy', # loss function, binary_crossentropy for binary classification\n",
    "              optimizer='sgd', # stochastic gradient descent\n",
    "              metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4a5a28-4bff-470b-8297-fcef27b12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # loss function, binary_crossentropy for binary classification\n",
    "              optimizer='sgd', # stochastic gradient descent\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cc11c-b81d-472d-b1e4-4674d72f9643",
   "metadata": {},
   "source": [
    "### 3. Train the model\n",
    "Train the model with the training data.\n",
    "```python\n",
    "x_train = np.random.random((1000, 100))\n",
    "y_train = np.random.randint(2, size=(1000, 10))\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06cd8d2-f6b2-4bfd-a075-5cb30e0292fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 821504832.0000 - accuracy: 0.1180\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 42366558208.0000 - accuracy: 0.1060\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2239328681984.0000 - accuracy: 0.1140\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 113362197282816.0000 - accuracy: 0.1190\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 6121284563894272.0000 - accuracy: 0.0920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163aab76320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.random.random((1000, 100))\n",
    "y_train = np.random.randint(2, size=(1000, 10))\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24939455-d0da-4580-962c-f85a683a980c",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model\n",
    "Evaluate the model with the test data.\n",
    "```python\n",
    "x_test = np.random.random((100, 100))\n",
    "y_test = np.random.randint(2, size=(100, 10))\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d193a409-8a8e-41d1-83bb-cb470c81edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step - loss: 48104123341471744.0000 - accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "x_test = np.random.random((100, 100))\n",
    "y_test = np.random.randint(2, size=(100, 10))\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085601a-ad0f-4e35-b204-a4e2cc72c9b5",
   "metadata": {},
   "source": [
    "## Keras LSTM for IMDB sentiment classification\n",
    "The IMDB dataset is in `datasets/` of this repository. Use the following code the load the dataset and write a LSTM model to classify the sentiment of the reviews.\n",
    "```python\n",
    "import pandas as pd    # to load dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords   # to get a collection of stopwords\n",
    "\n",
    "data = pd.read_csv('../datasets/IMDB.csv')\n",
    "\n",
    "custom_path = '../datasets/'\n",
    "\n",
    "# Append your custom path to the NLTK data path\n",
    "nltk.data.path.append(custom_path)\n",
    "\n",
    "nltk.download('stopwords', download_dir=custom_path)\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "x_data = data['review']       # Reviews/Input\n",
    "y_data = data['sentiment']    # Sentiment/Output\n",
    "# PRE-PROCESS REVIEW\n",
    "x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160833bb-10c4-496a-b483-01bf6df4451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b24dab8-5e86-4708-92de-99273307e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to dataset/...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords   # to get a collection of stopwords\n",
    "\n",
    "data = pd.read_csv('dataset/IMDB.csv')\n",
    "\n",
    "custom_path = 'dataset/'\n",
    "\n",
    "# Append your custom path to the NLTK data path\n",
    "nltk.data.path.append(custom_path)\n",
    "\n",
    "nltk.download('stopwords', download_dir=custom_path)\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "x_data = data['review']       # Reviews/Input\n",
    "y_data = data['sentiment']    # Sentiment/Output\n",
    "# PRE-PROCESS REVIEW\n",
    "x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c69f91-15ca-44ee-8415-cc1a0ba58391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22367208-ae65-4d74-90ee-c2a989f9068e",
   "metadata": {},
   "source": [
    "The tokenization of the reviews is done by the following code:\n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=10000)    # num_words is the number of words to keep based on word frequency\n",
    "tokenizer.fit_on_texts(x_data)            # fit tokenizer to our training text data\n",
    "\n",
    "# retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "x_data = tokenizer.texts_to_sequences(x_data)  # convert our text data to sequence of numbers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147e584f-7c5f-47cf-8eb0-75c97740e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [0 0 4 5]\n",
      " [6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example sequences\n",
    "sequences = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8, 9]\n",
    "]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe7c6a9-5878-4da9-b510-d9272206ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=10000)    # num_words is the number of words to keep based on word frequency\n",
    "tokenizer.fit_on_texts(x_data)            # fit tokenizer to our training text data\n",
    "\n",
    "# retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "x_data = tokenizer.texts_to_sequences(x_data)  # convert our text data to sequence of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c1bed0-23d1-4a1f-8484-7aee7912bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0, 4289, 1039,    5, 2538,   35, 1096,  347,\n",
       "           93,  172,  158,  722,  288,   23,  933,   91,   70,    3, 7536,\n",
       "            1,   79,  520,   70, 1451,  728,  262,  237,    4, 1513,   41,\n",
       "          354,   91,  127,   30,  758,    1,  332,  521,    1,  718,    4,\n",
       "           12, 1239, 9760, 3619,  204,   61,  642,   53,  260,  492,   73,\n",
       "         1195],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  119,\n",
       "            3,    1,  290,    1, 2124, 6264,   56,   98, 3198, 1801,  243,\n",
       "          113,  808,    2,   92,    2,   92,  108,   95,  431,   80,  819,\n",
       "          383,   35, 2259,    1,   57, 1336, 2081,  655,  447,   28,  533,\n",
       "         1853,  475,  218,  154,  137,  449, 2294,  569, 3502,  655,  983,\n",
       "           54,    1,  364,   17,  641, 2626,  198,   72,  620, 1574,    1,\n",
       "           40, 3994,  365,   29,  198,  211,    1,  119, 2051,  290,    1,\n",
       "          339,  229, 6265, 4550,  309,    4,    1, 1680,  458, 1326,  126,\n",
       "          140, 5945,  138,   13,   25,  566,  527,   16, 4228, 2081,   50,\n",
       "         2110]]),\n",
       " array([0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[10:12],y_data[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0299a-454b-41f2-89e8-12a82e969c45",
   "metadata": {},
   "source": [
    "Now, complete the following code to create a LSTM model for the IMDB sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d5fa0-1a69-4115-8965-7651243ab909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Dense, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_length = 100  # You can choose a different length\n",
    "x_data = pad_sequences(x_data, maxlen=max_length)\n",
    "\n",
    "# Convert sentiments to numerical labels\n",
    "y_data = np.where(y_data == 'positive', 1, 0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Build the RNN model\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "# Train the model\n",
    "\n",
    "## END OF YOUR CODE\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc32fecd-0a03-40c8-9dc6-7320266995ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 118s 115ms/step - loss: 0.3565 - accuracy: 0.8399 - val_loss: 0.2898 - val_accuracy: 0.8791\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.2244 - accuracy: 0.9119 - val_loss: 0.3049 - val_accuracy: 0.8744\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 107s 107ms/step - loss: 0.1728 - accuracy: 0.9336 - val_loss: 0.3443 - val_accuracy: 0.8609\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 107s 107ms/step - loss: 0.1357 - accuracy: 0.9503 - val_loss: 0.4201 - val_accuracy: 0.8668\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.1046 - accuracy: 0.9624 - val_loss: 0.4611 - val_accuracy: 0.8627\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 111s 111ms/step - loss: 0.0760 - accuracy: 0.9747 - val_loss: 0.4436 - val_accuracy: 0.8591\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 107s 107ms/step - loss: 0.0602 - accuracy: 0.9807 - val_loss: 0.4951 - val_accuracy: 0.8618\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 107s 107ms/step - loss: 0.0429 - accuracy: 0.9869 - val_loss: 0.5978 - val_accuracy: 0.8553\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 0.0394 - accuracy: 0.9877 - val_loss: 0.7232 - val_accuracy: 0.8550\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 105s 105ms/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.6873 - val_accuracy: 0.8584\n",
      "313/313 [==============================] - 19s 50ms/step - loss: 0.6861 - accuracy: 0.8559\n",
      "Test Accuracy: 0.85589998960495\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Dense, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_length = 100  # You can choose a different length\n",
    "x_data = pad_sequences(x_data, maxlen=max_length)\n",
    "\n",
    "# Convert sentiments to numerical labels\n",
    "y_data = np.where(y_data == 'positive', 1, 0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model architecture with LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=max_length),\n",
    "    LSTM(units=32),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce51eed-0eea-4f50-bfe9-110745830fd5",
   "metadata": {},
   "source": [
    "## Keras CNN for Digit Recognition\n",
    "In lab 5, we use the digit dataset. Now, we will use the same dataset to train a CNN model to recognize the digits.\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('../datasets/digits/Digits_X_train.csv').values\n",
    "y_train = pd.read_csv('../datasets/digits/Digits_y_train.csv').values\n",
    "X_test  = pd.read_csv('../datasets/digits/Digits_X_test.csv').values\n",
    "y_test  = pd.read_csv('../datasets/digits/Digits_y_test.csv').values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5789ec0b-d08a-41d3-84de-09a807d162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('dataset/digits/Digits_X_train.csv').values\n",
    "y_train = pd.read_csv('dataset/digits/Digits_y_train.csv').values\n",
    "X_test  = pd.read_csv('dataset/digits/Digits_X_test.csv').values\n",
    "y_test  = pd.read_csv('dataset/digits/Digits_y_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e96d5-1024-46c7-83a2-fa752fbb35e1",
   "metadata": {},
   "source": [
    "Complete the following code to create a CNN model for the digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56b310-a61b-464c-9aa0-9f608457a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, Flatten, MaxPooling2D\n",
    "\n",
    "# Reshape the data to 8 * 8 * 1\n",
    "X_train = X_train.reshape(X_train.shape[0], 8, 8, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 8, 8, 1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Create the model\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "# Train the model\n",
    "## END OF YOUR CODE\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5bef3e8-83fb-4757-85cd-08982650ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 64)          18496     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,426\n",
      "Trainable params: 28,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 1.3248 - accuracy: 0.6509 - val_loss: 0.5998 - val_accuracy: 0.8370\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.3365 - accuracy: 0.9304 - val_loss: 0.2498 - val_accuracy: 0.9444\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1720 - accuracy: 0.9517 - val_loss: 0.2013 - val_accuracy: 0.9481\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.1312 - accuracy: 0.9638 - val_loss: 0.1059 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.0770 - accuracy: 0.9777 - val_loss: 0.0744 - val_accuracy: 0.9852\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0737 - accuracy: 0.9805 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0629 - val_accuracy: 0.9852\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9926 - val_loss: 0.0571 - val_accuracy: 0.9815\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0322 - accuracy: 0.9926 - val_loss: 0.0561 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0285 - accuracy: 0.9944 - val_loss: 0.0378 - val_accuracy: 0.9926\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9822\n",
      "Accuracy:  0.9822221994400024\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, Flatten, MaxPooling2D\n",
    "\n",
    "# Reshape the data to 8 * 8 * 1\n",
    "X_train = X_train.reshape(X_train.shape[0], 8, 8, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 8, 8, 1)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')  # 10 output units for 10 digits\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e815cf-20d3-4e20-a769-79c508dcee02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
